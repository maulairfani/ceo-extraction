{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T06:45:00.913433Z",
     "iopub.status.busy": "2025-01-19T06:45:00.913163Z",
     "iopub.status.idle": "2025-01-19T06:45:23.260019Z",
     "shell.execute_reply": "2025-01-19T06:45:23.259156Z",
     "shell.execute_reply.started": "2025-01-19T06:45:00.913411Z"
    },
    "id": "PBWZoqpUljgy",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maula\\Desktop\\CESGS\\CEO - Copy\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "import PyPDF2\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import uuid\n",
    "import glob\n",
    "import json\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import subprocess\n",
    "import os\n",
    "from urllib.parse import urlparse, unquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T06:45:23.261641Z",
     "iopub.status.busy": "2025-01-19T06:45:23.260946Z",
     "iopub.status.idle": "2025-01-19T06:45:23.273460Z",
     "shell.execute_reply": "2025-01-19T06:45:23.272603Z",
     "shell.execute_reply.started": "2025-01-19T06:45:23.261610Z"
    },
    "id": "VEl3PdUAlk8-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PreprocessorResponse:\n",
    "    def __init__(self, results: List[Dict[str, Any]]):\n",
    "        self.results = results\n",
    "\n",
    "    def save_to_json(self, output_file_path: str):\n",
    "        try:\n",
    "            # Ensure the directory exists\n",
    "            directory = os.path.dirname(output_file_path)\n",
    "            if directory and not os.path.exists(directory):\n",
    "                print(f\"Creating directory: {directory}\")\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            # Save the results to the JSON file\n",
    "            with open(output_file_path, 'w') as json_file:\n",
    "                json.dump(self.results, json_file, indent=4)\n",
    "                print(f\"Results saved to {output_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to JSON: {e}\")\n",
    "\n",
    "class Preprocessor:\n",
    "    def get_pdf_page_count(self, pdf_path: str) -> int:\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as pdf_file:\n",
    "                reader = PyPDF2.PdfReader(pdf_file)\n",
    "                page_count = len(reader.pages)\n",
    "                return page_count\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while counting pages: {e}\")\n",
    "            return -1\n",
    "\n",
    "    def extract_page(self, input_pdf_path: str, output_pdf_path: str, page_number: int):\n",
    "        try:\n",
    "            with open(input_pdf_path, 'rb') as input_pdf:\n",
    "                reader = PyPDF2.PdfReader(input_pdf)\n",
    "                writer = PyPDF2.PdfWriter()\n",
    "\n",
    "                if page_number < 0 or page_number >= len(reader.pages):\n",
    "                    raise ValueError(\"Invalid page number.\")\n",
    "\n",
    "                writer.add_page(reader.pages[page_number])\n",
    "\n",
    "                with open(output_pdf_path, 'wb') as output_pdf:\n",
    "                    writer.write(output_pdf)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while extracting page: {e}\")\n",
    "\n",
    "    def process(self, file_path: str):\n",
    "        page_count = self.get_pdf_page_count(file_path)\n",
    "        if page_count == -1:\n",
    "            raise ValueError(\"Failed to get the page count of the PDF.\")\n",
    "\n",
    "        results = []\n",
    "        for i in tqdm(range(page_count)):\n",
    "            temp_pdf_path = \"temp.pdf\"\n",
    "            self.extract_page(file_path, temp_pdf_path, i)\n",
    "\n",
    "            try:\n",
    "                pipeline_options = PdfPipelineOptions()\n",
    "                pipeline_options.do_ocr = False  # Disable OCR\n",
    "                pipeline_options.accelerator_options.num_threads = 20\n",
    "\n",
    "                converter = DocumentConverter(\n",
    "                    format_options={\n",
    "                        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "                    }\n",
    "                )\n",
    "                result = converter.convert(temp_pdf_path)\n",
    "                result_md = result.document.export_to_markdown()\n",
    "\n",
    "                results.append({\n",
    "                    \"page_content\": result_md,\n",
    "                    \"metadata\": {\n",
    "                        \"page\": i + 1,\n",
    "                    },\n",
    "                    \"id\": str(uuid.uuid4())\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while converting page {i + 1}: {e}\")\n",
    "\n",
    "        return PreprocessorResponse(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T06:45:23.275403Z",
     "iopub.status.busy": "2025-01-19T06:45:23.275161Z",
     "iopub.status.idle": "2025-01-19T06:45:23.413860Z",
     "shell.execute_reply": "2025-01-19T06:45:23.413077Z",
     "shell.execute_reply.started": "2025-01-19T06:45:23.275383Z"
    },
    "id": "Trd5Zj_xlq-j",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "def folder_to_zip(folder_path, output_zip_path):\n",
    "    try:\n",
    "        # Menggunakan shutil.make_archive untuk membuat zip\n",
    "        zip_path = shutil.make_archive(output_zip_path, 'zip', folder_path)\n",
    "        print(f\"Folder berhasil diubah menjadi ZIP: {zip_path}\")\n",
    "        return zip_path\n",
    "    except Exception as e:\n",
    "        print(f\"Terjadi kesalahan: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T06:45:23.415262Z",
     "iopub.status.busy": "2025-01-19T06:45:23.414965Z",
     "iopub.status.idle": "2025-01-19T06:45:23.420906Z",
     "shell.execute_reply": "2025-01-19T06:45:23.420128Z",
     "shell.execute_reply.started": "2025-01-19T06:45:23.415235Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ADJUSTABLE\n",
    "\n",
    "urls = [\n",
    "  'https://storage.googleapis.com/cesgs-dart/TCFD%20Report/AR/2020/HK_0002_AR_2020(1).pdf',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-19T06:50:38.948Z",
     "iopub.execute_input": "2025-01-19T06:45:23.422055Z",
     "iopub.status.busy": "2025-01-19T06:45:23.421733Z"
    },
    "id": "OBLkRc5OmIv7",
    "outputId": "912b774e-6c1c-42bf-8892-ff53cff25700",
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memproses (1/1) - https://storage.googleapis.com/cesgs-dart/TCFD%20Report/AR/2020/HK_0002_AR_2020(1).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to data\\JSON\\TCFD Report\\AR\\2020\\HK_0002_AR_2020(1).json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "failed = []\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    print(f\"Memproses ({idx + 1}/{len(urls)}) - {url}\", flush=True)\n",
    "    \n",
    "    try:\n",
    "        # 1. Download file\n",
    "        encoded_url = url.replace(\"(\", \"%28\").replace(\")\", \"%29\")\n",
    "        result = subprocess.run(\n",
    "            [\"wget\", \"-q\", encoded_url],\n",
    "            check=True,\n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "        \n",
    "        # 2. Proses file\n",
    "        preprocessor = Preprocessor()\n",
    "        pdf_filename = unquote(os.path.basename(urlparse(url).path))\n",
    "        pdf_path = os.path.join(pdf_filename)\n",
    "        \n",
    "        # 3. Ekstraksi dan simpan output\n",
    "        response = preprocessor.process(pdf_path)\n",
    "        output_path = os.path.normpath(os.path.join(\n",
    "            \"data\", \"JSON\",\n",
    "            url.replace(\"https://storage.googleapis.com/cesgs-dart/\", \"\")[:-4].replace(\"%20\", \" \") + '.json'\n",
    "        ))\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        response.save_to_json(output_path)\n",
    "        \n",
    "        # 4. Bersihkan file PDF\n",
    "        os.remove(pdf_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Gagal memproses {url}: {str(e)}\")\n",
    "        failed.append(url)\n",
    "\n",
    "# Simpan daftar yang gagal ke CSV\n",
    "if failed:\n",
    "    pd.Series(failed).to_csv(\"failed.csv\", index=False)\n",
    "    print(f\"{len(failed)} URL gagal, disimpan ke failed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ADJUSTABLE\n",
    "pdf_dir = \"data\\\\PDF\\\\\"\n",
    "files = glob.glob(pdf_dir + \"*\\*.pdf\")\n",
    "files = [os.path.normpath(f) for f in files]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memproses (1/1) - data\\PDF\\AR\\ID_ADRO_AR_2022.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 504/504 [26:50<00:00,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to data\\JSON\\AR\\ID_ADRO_AR_2022.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "failed = []\n",
    "\n",
    "for idx, pdf_path in enumerate(files):\n",
    "    print(f\"Memproses ({idx + 1}/{len(files)}) - {pdf_path}\", flush=True)\n",
    "    \n",
    "    try:\n",
    "        # 2. Proses file\n",
    "        preprocessor = Preprocessor()\n",
    "        \n",
    "        # 3. Ekstraksi dan simpan output\n",
    "        response = preprocessor.process(pdf_path)\n",
    "        output_path = os.path.normpath(os.path.join(\n",
    "            \"data\", \"JSON\",\n",
    "            pdf_path.replace(pdf_dir, \"\")[:-4].replace(\"%20\", \" \") + '.json'\n",
    "        ))\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        response.save_to_json(output_path)\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"Gagal memproses {pdf_path}: {str(e)}\")\n",
    "        failed.append(pdf_path)\n",
    "\n",
    "# Simpan daftar yang gagal ke CSV\n",
    "if failed:\n",
    "    pd.Series(failed).to_csv(\"failed.csv\", index=False)\n",
    "    print(f\"{len(failed)} file gagal, disimpan ke failed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6492539,
     "sourceId": 10486104,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6492577,
     "sourceId": 10486158,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
